{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc2c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# This notebook defines a Delta Live Tables pipeline focusing on the Raw layer\n",
    "# following the Raw, Conformed, Curated architecture using batch processing.\n",
    "# Data is ingested from Unity Catalog Volumes, and partitioning is applied to raw_orders.\n",
    "\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, DateType\n",
    "\n",
    "# Define the base Unity Catalog Volume path where your raw data resides\n",
    "UC_RAW_INBOUND_PATH = \"/Volumes/dbndev/raw/inbound\"\n",
    "\n",
    "# Define specific paths for each entity's raw data within the UC Volume\n",
    "# For orders, point to the directory containing monthly/daily files\n",
    "RAW_ORDERS_PATH = f\"{UC_RAW_INBOUND_PATH}/orders/\"\n",
    "# For customers and products, point directly to the single JSON file\n",
    "RAW_CUSTOMERS_PATH = f\"{UC_RAW_INBOUND_PATH}/customers/customer_data.json\"\n",
    "RAW_PRODUCTS_PATH = f\"{UC_RAW_INBOUND_PATH}/products/product_data.json\"\n",
    "\n",
    "# Define explicit schema for raw orders to prevent _corrupt_record issues with empty/malformed files\n",
    "# This ensures Spark knows the expected columns, even if data is not present or is corrupt.\n",
    "orders_explicit_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True), # Read as String, then cast later if needed\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"total_price\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "    # Add any other expected fields from your raw JSON orders here\n",
    "])\n",
    "\n",
    "# Define explicit schema for raw customers\n",
    "customers_explicit_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"registration_date\", StringType(), True) # Read as String, then cast later\n",
    "])\n",
    "\n",
    "# Define explicit schema for raw products\n",
    "products_explicit_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"stock_quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# COMMAND ----------\n",
    "# RAW LAYER TABLES (Ingestion from raw sources)\n",
    "# These tables will reside in the 'raw' schema when deployed (configured via 'target' in DLT settings)\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"raw_orders\",\n",
    "    comment=\"Raw customer order data ingested from UC Volume (Raw Layer), partitioned by source date.\",\n",
    "    table_properties={\"quality\": \"raw\"}\n",
    ")\n",
    "def raw_orders():\n",
    "    \"\"\"\n",
    "    Reads raw JSON order data as a batch from the specified UC Volume path.\n",
    "    Uses an explicit schema to handle potentially empty or malformed files,\n",
    "    extracts date from filename to use for partitioning (YYYY/MM/DD folder structure).\n",
    "    - For monthly full loads (e.g., monthly_full_load_2025_01.json), uses YYYY-MM-01.\n",
    "    - For daily incremental loads (e.g., orders_2025_06_15.json), uses YYYY-MM-DD.\n",
    "    \"\"\"\n",
    "    # Use the explicit schema when reading the JSON files\n",
    "    df = spark.read.format(\"json\").schema(orders_explicit_schema).load(RAW_ORDERS_PATH)\n",
    "    \n",
    "    # Use _metadata.file_path as recommended by Unity Catalog and extract the filename\n",
    "    df_with_filepath = df.withColumn(\"source_filepath\", F.col(\"_metadata.file_path\"))\n",
    "    df_with_filename = df_with_filepath.withColumn(\"actual_filename\", F.substring_index(F.col(\"source_filepath\"), '/', -1))\n",
    "\n",
    "    # Extract date from filename based on patterns\n",
    "    # Pattern for daily files: orders_YYYY_MM_DD.json\n",
    "    # Pattern for monthly full load files: monthly_full_load_YYYY_MM.json\n",
    "    df_with_date = df_with_filename.withColumn(\n",
    "        \"file_date_str\",\n",
    "        F.when(\n",
    "            F.col(\"actual_filename\").contains(\"orders_\"),\n",
    "            F.regexp_extract(F.col(\"actual_filename\"), r\"orders_(\\d{4}_\\d{2}_\\d{2})\\.json\", 1)\n",
    "        ).when(\n",
    "            F.col(\"actual_filename\").contains(\"monthly_full_load_\"),\n",
    "            F.regexp_extract(F.col(\"actual_filename\"), r\"monthly_full_load_(\\d{4}_\\d{2})\\.json\", 1) + \"_01\" # Add _01 for day\n",
    "        ).otherwise(\n",
    "            F.lit(\"UNKNOWN\") # Handle unexpected filenames, or use current date or drop\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert the extracted string date to a proper DateType for partitioning\n",
    "    # Replace underscores with hyphens for parsing if needed, then cast\n",
    "    return df_with_date.withColumn(\n",
    "        \"file_date\",\n",
    "        F.to_date(F.regexp_replace(F.col(\"file_date_str\"), \"_\", \"-\"), \"yyyy-MM-dd\").cast(DateType())\n",
    "    ).drop(\"source_filepath\", \"actual_filename\", \"file_date_str\") # Drop intermediate columns\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"raw_customers\",\n",
    "    comment=\"Raw customer master data ingested from UC Volume (Raw Layer).\",\n",
    "    table_properties={\"quality\": \"raw\"}\n",
    ")\n",
    "def raw_customers():\n",
    "    \"\"\"Reads raw JSON customer data as a batch from the specified UC Volume file.\"\"\"\n",
    "    # Use the explicit schema when reading the JSON files\n",
    "    return spark.read.format(\"json\").schema(customers_explicit_schema).load(RAW_CUSTOMERS_PATH)\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"raw_products\",\n",
    "    comment=\"Raw product catalog data ingested from UC Volume (Raw Layer).\",\n",
    "    table_properties={\"quality\": \"raw\"}\n",
    ")\n",
    "def raw_products():\n",
    "    \"\"\"Reads raw JSON product data as a batch from the specified UC Volume file.\"\"\"\n",
    "    # Use the explicit schema when reading the JSON files\n",
    "    return spark.read.format(\"json\").schema(products_explicit_schema).load(RAW_PRODUCTS_PATH)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "# CONFORMED LAYER TABLES (Cleaning, Transformation, Conformance)\n",
    "# These tables are commented out as requested. We will implement SCD Type 2 and other\n",
    "# transformations here later.\n",
    "\n",
    "# @dlt.table(\n",
    "#     name=\"conformed_dim_customer_product\",\n",
    "#     comment=\"Cleaned and conformed customer and product dimension data (Conformed Layer).\",\n",
    "#     table_properties={\"quality\": \"conformed\", \"delta.auto.optimize.optimizeWrite\": \"true\"}\n",
    "# )\n",
    "# @dlt.expect_or_drop(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "# @dlt.expect_or_drop(\"valid_product_id\", \"product_id IS NOT NULL\")\n",
    "# def conformed_dim_customer_product():\n",
    "#     customers_df = dlt.read(\"raw_customers\")\n",
    "#     products_df = dlt.read(\"raw_products\")\n",
    "#     # Add your join and transformation logic here for customer and product dimensions\n",
    "#     return (\n",
    "#         customers_df\n",
    "#         .join(products_df, customers_df[\"customer_id\"] == products_df[\"product_id\"], \"fullouter\")\n",
    "#         .select(\n",
    "#             F.col(\"customer_id\").alias(\"dim_customer_id\"),\n",
    "#             F.col(\"first_name\").alias(\"customer_first_name\"),\n",
    "#             F.col(\"last_name\").alias(\"customer_last_name\"),\n",
    "#             F.col(\"email\").alias(\"customer_email\"),\n",
    "#             F.col(\"city\").alias(\"customer_city\"),\n",
    "#             F.col(\"country\").alias(\"customer_country\"),\n",
    "#             F.col(\"registration_date\").alias(\"customer_registration_date\"),\n",
    "#             F.col(\"product_id\").alias(\"dim_product_id\"),\n",
    "#             F.col(\"product_name\").alias(\"product_name\"),\n",
    "#             F.col(\"category\").alias(\"product_category\"),\n",
    "#             F.col(\"price\").alias(\"product_unit_price\"),\n",
    "#             F.col(\"stock_quantity\").alias(\"product_stock_quantity\")\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# @dlt.table(\n",
    "#     name=\"conformed_fact_sales\",\n",
    "#     comment=\"Cleaned and validated customer order fact data (Conformed Layer).\",\n",
    "#     table_properties={\"quality\": \"conformed\", \"delta.auto.optimize.optimizeWrite\": \"true\"}\n",
    "# )\n",
    "# @dlt.expect_or_drop(\"order_id_not_null\", \"order_id IS NOT NULL\")\n",
    "# @dlt.expect_or_fail(\"customer_id_not_null\", \"customer_id IS NOT NULL\")\n",
    "# @dlt.expect(\"valid_order_date\", \"order_datetime IS NOT NULL AND order_datetime >= '2020-01-01'\")\n",
    "# @dlt.expect(\"positive_quantity\", \"quantity > 0\")\n",
    "# def conformed_fact_sales():\n",
    "#     orders_df = dlt.read(\"raw_orders\")\n",
    "#     # conformed_dim_df = dlt.read(\"conformed_dim_customer_product\") # Reads from the conformed dimension\n",
    "#     cleaned_orders_df = (\n",
    "#         orders_df\n",
    "#         .select(\n",
    "#             F.col(\"customer_id\").cast(StringType()).alias(\"customer_id\"),\n",
    "#             F.col(\"order_id\").cast(StringType()).alias(\"order_id\"),\n",
    "#             F.col(\"order_date\").cast(TimestampType()).alias(\"order_datetime\"),\n",
    "#             F.col(\"product_id\").cast(StringType()).alias(\"product_id\"),\n",
    "#             F.col(\"quantity\").cast(IntegerType()).alias(\"quantity\"),\n",
    "#             F.col(\"total_price\").cast(DoubleType()).alias(\"total_item_price_usd\"),\n",
    "#             F.col(\"status\").cast(StringType()).alias(\"order_status\")\n",
    "#         )\n",
    "#     )\n",
    "#     # Add join with dimension data for enrichment here\n",
    "#     # return (\n",
    "#     #     cleaned_orders_df\n",
    "#     #     .join(conformed_dim_df, ...)\n",
    "#     #     .select(...)\n",
    "#     # )\n",
    "#     return cleaned_orders_df # Returning cleaned_orders_df for now, will be replaced later\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "# CURATED LAYER TABLES (Aggregations, Business Views)\n",
    "# These tables are commented out as requested.\n",
    "\n",
    "# @dlt.table(\n",
    "#     name=\"curated_daily_sales_summary\",\n",
    "#     comment=\"Aggregated daily sales data (Curated Layer).\",\n",
    "#     table_properties={\"quality\": \"curated\"}\n",
    "# )\n",
    "# def curated_daily_sales_summary():\n",
    "#     # return dlt.read(\"conformed_fact_sales\").groupBy(...).agg(...)\n",
    "#     pass # Placeholder for future implementation\n",
    "\n",
    "# @dlt.table(\n",
    "#     name=\"curated_monthly_customer_spending\",\n",
    "#     comment=\"Aggregated monthly customer spending (Curated Layer).\",\n",
    "#     table_properties={\"quality\": \"curated\"}\n",
    "# )\n",
    "# def curated_monthly_customer_spending():\n",
    "#     # return dlt.read(\"conformed_fact_sales\").groupBy(...).agg(...)\n",
    "#     pass # Placeholder for future implementation\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
