# databricks.yaml
# Bundle definition for Retail DLT Pipeline.

bundle:
  name: retail-dlt-pipeline

resources:
  pipelines:
    retail_raw_ingestion_pipeline:
      name: ${bundle.target}-retail-raw-ingestion-pipeline # Pipeline name in Databricks UI
      # storage_location removed: DLT manages storage location automatically with Unity Catalog target.
      libraries:
        - notebook: # Changed to 'notebook' to indicate a Databricks notebook file
            path: src/retail_dlt.ipynb # Path to your DLT notebook file
      target: raw # Tables published to 'raw' schema in Unity Catalog
      catalog: dbndev # Explicitly set Unity Catalog catalog
      serverless: true # Explicitly enable serverless compute for the DLT pipeline
      # Removed 'clusters' block entirely, as 'serverless: true' makes it redundant and
      # could cause conflicts with workspace-level serverless requirements.
      continuous: false # Batch pipeline
      development: true # Set to false for production deployments
      photon: true # Enabled Photon, essential for Serverless DLT in most configurations
      channel: CURRENT # Use latest DLT runtime

  jobs:
    run_retail_dlt_job:
      name: ${bundle.target}-run-retail-raw-dlt-job
      tasks:
        - task_key: run_dlt_pipeline
          pipeline_task:
            pipeline_id: ${resources.pipelines.retail_raw_ingestion_pipeline.id} # Reference to DLT pipeline
      # Schedule block remains removed as requested earlier.

targets:
  dev:
    workspace:
      host: https://***.cloud.databricks.com/ # Your Databricks workspace URL
