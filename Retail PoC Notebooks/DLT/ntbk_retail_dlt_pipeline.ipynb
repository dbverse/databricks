{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "315409a2-204f-4710-b0e8-babc10199f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# This notebook defines a Delta Live Tables pipeline focusing on the Raw layer\n",
    "# following the Raw, Conformed, Curated architecture using batch processing.\n",
    "# Data is ingested from Unity Catalog Volumes, and partitioning is applied to raw_orders.\n",
    "\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, DateType\n",
    "\n",
    "# Define the base Unity Catalog Volume path where your raw data resides\n",
    "UC_RAW_INBOUND_PATH = \"/Volumes/dbndev/raw/inbound\"\n",
    "\n",
    "# Define specific paths for each entity's raw data within the UC Volume\n",
    "# For orders, point to the directory containing monthly/daily files\n",
    "RAW_ORDERS_PATH = f\"{UC_RAW_INBOUND_PATH}/orders/\"\n",
    "# For customers and products, point directly to the single JSON file\n",
    "RAW_CUSTOMERS_PATH = f\"{UC_RAW_INBOUND_PATH}/customers/customer_data.json\"\n",
    "RAW_PRODUCTS_PATH = f\"{UC_RAW_INBOUND_PATH}/products/product_data.json\"\n",
    "\n",
    "# COMMAND ----------\n",
    "# RAW LAYER TABLES (Ingestion from raw sources)\n",
    "# These tables will reside in the 'raw' schema when deployed (configured via 'target' in DLT settings)\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"raw_orders\", # Renamed from bronze_orders\n",
    "    comment=\"Raw customer order data ingested from UC Volume (Raw Layer), partitioned by source date.\",\n",
    "    table_properties={\"quality\": \"raw\"}, # Renamed from bronze\n",
    "    partition_by=[\"file_date\"] # Partition by the derived file_date column\n",
    ")\n",
    "def raw_orders():\n",
    "    \"\"\"\n",
    "    Reads raw JSON order data as a batch from the specified UC Volume path.\n",
    "    Extracts date from filename to use for partitioning (YYYY/MM/DD folder structure).\n",
    "    - For monthly full loads (e.g., monthly_full_load_2025_01.json), uses YYYY-MM-01.\n",
    "    - For daily incremental loads (e.g., orders_2025_06_15.json), uses YYYY-MM-DD.\n",
    "    \"\"\"\n",
    "    df = spark.read.format(\"json\").load(RAW_ORDERS_PATH)\n",
    "    \n",
    "    # Add a column for the source filename to extract date\n",
    "    df_with_filename = df.withColumn(\"source_filename\", F.input_file_name())\n",
    "\n",
    "    # Extract date from filename based on patterns\n",
    "    # Pattern for daily files: orders_YYYY_MM_DD.json\n",
    "    # Pattern for monthly full load files: monthly_full_load_YYYY_MM.json\n",
    "    df_with_date = df_with_filename.withColumn(\n",
    "        \"file_date_str\",\n",
    "        F.when(\n",
    "            F.col(\"source_filename\").contains(\"orders_\"),\n",
    "            F.regexp_extract(F.col(\"source_filename\"), r\"orders_(\\d{4}_\\d{2}_\\d{2})\\.json\", 1)\n",
    "        ).when(\n",
    "            F.col(\"source_filename\").contains(\"monthly_full_load_\"),\n",
    "            F.regexp_extract(F.col(\"source_filename\"), r\"monthly_full_load_(\\d{4}_\\d{2})\\.json\", 1) + \"_01\" # Add _01 for day\n",
    "        ).otherwise(\n",
    "            F.lit(\"UNKNOWN\") # Handle unexpected filenames, or use current date or drop\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert the extracted string date to a proper DateType for partitioning\n",
    "    # Replace underscores with hyphens for parsing if needed, then cast\n",
    "    return df_with_date.withColumn(\n",
    "        \"file_date\",\n",
    "        F.to_date(F.regexp_replace(F.col(\"file_date_str\"), \"_\", \"-\"), \"yyyy-MM-dd\").cast(DateType())\n",
    "    ).drop(\"source_filename\", \"file_date_str\") # Drop intermediate columns\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"raw_customers\", # Renamed from bronze_customers\n",
    "    comment=\"Raw customer master data ingested from UC Volume (Raw Layer).\",\n",
    "    table_properties={\"quality\": \"raw\"} # Renamed from bronze\n",
    ")\n",
    "def raw_customers():\n",
    "    \"\"\"Reads raw JSON customer data as a batch from the specified UC Volume file.\"\"\"\n",
    "    # Assuming 'customer_data.json' contains all customer records\n",
    "    return spark.read.format(\"json\").load(RAW_CUSTOMERS_PATH)\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"raw_products\", # Renamed from bronze_products\n",
    "    comment=\"Raw product catalog data ingested from UC Volume (Raw Layer).\",\n",
    "    table_properties={\"quality\": \"raw\"} # Renamed from bronze\n",
    ")\n",
    "def raw_products():\n",
    "    \"\"\"Reads raw JSON product data as a batch from the specified UC Volume file.\"\"\"\n",
    "    # Assuming 'product_data.json' contains all product records\n",
    "    return spark.read.format(\"json\").load(RAW_PRODUCTS_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ecdf53c-12b6-4b69-b239-5ade281dd7b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# CONFORMED LAYER TABLES (Cleaning, Transformation, Conformance)\n",
    "# These tables are commented out as requested. We will implement SCD Type 2 and other\n",
    "# transformations here later.\n",
    "\n",
    "# @dlt.table(\n",
    "#     name=\"conformed_dim_customer_product\",\n",
    "#     comment=\"Cleaned and conformed customer and product dimension data (Conformed Layer).\",\n",
    "#     table_properties={\"quality\": \"conformed\", \"delta.auto.optimize.optimizeWrite\": \"true\"}\n",
    "# )\n",
    "# @dlt.expect_or_drop(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "# @dlt.expect_or_drop(\"valid_product_id\", \"product_id IS NOT NULL\")\n",
    "# def conformed_dim_customer_product():\n",
    "#     customers_df = dlt.read(\"raw_customers\")\n",
    "#     products_df = dlt.read(\"raw_products\")\n",
    "#     # Add your join and transformation logic here for customer and product dimensions\n",
    "#     return (\n",
    "#         customers_df\n",
    "#         .join(products_df, customers_df[\"customer_id\"] == products_df[\"product_id\"], \"fullouter\")\n",
    "#         .select(\n",
    "#             F.col(\"customer_id\").alias(\"dim_customer_id\"),\n",
    "#             F.col(\"first_name\").alias(\"customer_first_name\"),\n",
    "#             F.col(\"last_name\").alias(\"customer_last_name\"),\n",
    "#             F.col(\"email\").alias(\"customer_email\"),\n",
    "#             F.col(\"city\").alias(\"customer_city\"),\n",
    "#             F.col(\"country\").alias(\"customer_country\"),\n",
    "#             F.col(\"registration_date\").alias(\"customer_registration_date\"),\n",
    "#             F.col(\"product_id\").alias(\"dim_product_id\"),\n",
    "#             F.col(\"product_name\").alias(\"product_name\"),\n",
    "#             F.col(\"category\").alias(\"product_category\"),\n",
    "#             F.col(\"price\").alias(\"product_unit_price\"),\n",
    "#             F.col(\"stock_quantity\").alias(\"product_stock_quantity\")\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# @dlt.table(\n",
    "#     name=\"conformed_fact_sales\",\n",
    "#     comment=\"Cleaned and validated customer order fact data (Conformed Layer).\",\n",
    "#     table_properties={\"quality\": \"conformed\", \"delta.auto.optimize.optimizeWrite\": \"true\"}\n",
    "# )\n",
    "# @dlt.expect_or_drop(\"order_id_not_null\", \"order_id IS NOT NULL\")\n",
    "# @dlt.expect_or_fail(\"customer_id_not_null\", \"customer_id IS NOT NULL\")\n",
    "# @dlt.expect(\"valid_order_date\", \"order_datetime IS NOT NULL AND order_datetime >= '2020-01-01'\")\n",
    "# @dlt.expect(\"positive_quantity\", \"quantity > 0\")\n",
    "# def conformed_fact_sales():\n",
    "#     orders_df = dlt.read(\"raw_orders\")\n",
    "#     # conformed_dim_df = dlt.read(\"conformed_dim_customer_product\") # Reads from the conformed dimension\n",
    "#     cleaned_orders_df = (\n",
    "#         orders_df\n",
    "#         .select(\n",
    "#             F.col(\"customer_id\").cast(StringType()).alias(\"customer_id\"),\n",
    "#             F.col(\"order_id\").cast(StringType()).alias(\"order_id\"),\n",
    "#             F.col(\"order_date\").cast(TimestampType()).alias(\"order_datetime\"),\n",
    "#             F.col(\"product_id\").cast(StringType()).alias(\"product_id\"),\n",
    "#             F.col(\"quantity\").cast(IntegerType()).alias(\"quantity\"),\n",
    "#             F.col(\"total_price\").cast(DoubleType()).alias(\"total_item_price_usd\"),\n",
    "#             F.col(\"status\").cast(StringType()).alias(\"order_status\")\n",
    "#         )\n",
    "#     )\n",
    "#     # Add join with dimension data for enrichment here\n",
    "#     # return (\n",
    "#     #     cleaned_orders_df\n",
    "#     #     .join(conformed_dim_df, ...)\n",
    "#     #     .select(...)\n",
    "#     # )\n",
    "#     return cleaned_orders_df # Returning cleaned_orders_df for now, will be replaced later\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4df2304e-3cd4-4c82-88bb-450b04dc4dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# CURATED LAYER TABLES (Aggregations, Business Views)\n",
    "# These tables are commented out as requested.\n",
    "\n",
    "# @dlt.table(\n",
    "#     name=\"curated_daily_sales_summary\",\n",
    "#     comment=\"Aggregated daily sales data (Curated Layer).\",\n",
    "#     table_properties={\"quality\": \"curated\"}\n",
    "# )\n",
    "# def curated_daily_sales_summary():\n",
    "#     # return dlt.read(\"conformed_fact_sales\").groupBy(...).agg(...)\n",
    "#     pass # Placeholder for future implementation\n",
    "\n",
    "# @dlt.table(\n",
    "#     name=\"curated_monthly_customer_spending\",\n",
    "#     comment=\"Aggregated monthly customer spending (Curated Layer).\",\n",
    "#     table_properties={\"quality\": \"curated\"}\n",
    "# )\n",
    "# def curated_monthly_customer_spending():\n",
    "#     # return dlt.read(\"conformed_fact_sales\").groupBy(...).agg(...)\n",
    "#     pass # Placeholder for future implementation\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ntbk_retail_dlt_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
